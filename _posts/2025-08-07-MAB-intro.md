---
layout: post
title: Introduction to Bandit Algorithm
date: 2025-08-07 17:15 +0900
categories: [Reinforcement Learning, Theory]
tags: [rl, reinforcement, learning, machine_learning, ai, ml, theory]
description: Multi Armed Bandit 상황 설명
---

# What is Multi-Armed Bandit(MAB) problem?
- 가장 기초적인 MAB는 다음 상황이다
	"
	당신은 카지노에서 룰렛 게임을 하려고 한다. 버튼은 A, B 두 개가 있고, 버튼을 눌렀을 때 나오는 숫자만큼 당신은 돈을 가져갈 수 있다. 어떻게 할 것인가?
	"
- 이에 대한 대답은 전략(strategy)이어야 한다.
- 너무 막막한 사람들을 위해 조금 더 질문을 늘리면
	"
	시험삼아 당신은 A버튼을 5번, B버튼을 5번 눌러 보았다. 그 때 A버튼에서는 (0,10,0,0,10) 이 나왔고, B버튼에선 (10,0,0,0,0) 이 나왔다. 그 다음에는 당신은 A버튼을 누를 것인가, B버튼을 누를 것인가? A와 B를 번갈아 누를 것인가? 2:1의 비율로 누를 것인가?
	"
- 여기에서 짐작할 수 있듯, MAB에 대한 전략으로는 크게 두 방향이 있다
	1. Exploration(탐험) : 정보를 얻기 위해 시험 삼아 해보기.
	2. Exploitation(착취) : 최선의 결과를 얻기 위해 하기
- 이 둘 간의 Balance를 찾는 것이 Bandit problem의 핵심이다.
- 참고로, 외국에서는 룰렛을 돈을 빼앗아간다는 의미로 bandit(도둑)이라고도 쓴다. 룰렛에 누를 수 있는 버튼(또는 레버)이 두 개 일 경우 double-arm이라고 한다. 즉, 위의 상황은 Double-armed bandit 상황인 것이다. 이걸 확장하여 내가 선택할 수 있는 게 많은 경우 Multi-armed bandit problem이라고 부른다.

# Bandit problem Modelling
- MAB는 한줄로 말하면 "sequential game between a learner and an environment"다. 위 상황에 대입해보면 environment는 룰렛기계(MAB)를 말하는 거고, learner는 당신이다. 좀더 깊이 파보면,
	- Learner는 이전 기록(History)에만 의존하여 결정을 내릴 수 있다. 
	- Learner의 '전략'을 policy $\pi$ 라 부르며, 이 policy는 History에서 action으로 가는 mapping 이다.
	- Environment는 이번 action까지의 History에서 reward로 가는 mapping이다.
		- True environment는 some environment class set $\mathcal{E}$ 에 속한다고 가정한다.
		- theoretical MAB problem은 environment class에 따른 policy를 어떻게 design하는가 라고 볼 수 있다.
	- 아래 그림의 과정을 계속 반복하는데, 반복하는 횟수를 horizon이라고 한다. 보통 horizon을 n으로 표기하고, each round는 $t \in [n]$ 이라 표기한다.
- 정리하면 아래 그림과 같다. 
![[assets/img/postimgs/MAB_structure.png]]


# 어떤 Learner가 좋은 learner인가?
- Learner를 평가하는 방법으로, **regret**은 좋은 지표이다.

Definition 1. (엄밀) The regret of learner relative to a policy $\pi$ = (total expected reward using policy $\pi$ for n rounds) - (total expected reward collected by the learner over n rounds)

Definition 1. (느낌) $\pi$에 대한 regret = ($\pi$로 했을 때 리워드) - (내 policy로 했을 때 리워드)

Definition 2. (엄밀) The regret of learner relative to a set of policies $\Pi$ = $\max\limits_{\pi \in \Pi} \text{regret}(\pi)$  

Definition 2. (느낌) regret = optimal reward - learner reward = $\pi$ 가 여러개 될 수 있는데 그중 maximum reward - 내 policy의 reward

- $\Pi$ is called the competitor class.
- regret은 performance of the learner relative to the best policy in the competitor class라고 할 수 있음.
- global optimal과의 차이는 더 큼 (best policy in the competitor class != answer) . 그러나 확률적으로 reward가 나오는 상황에서 매번 정답을 맞추는 것은 사실상 불가능. 그러므로 best policy와 비교하는 게 맞음.
- 특정 policy로 고정한 상황에선 environment에 따라 regret이 변함. 

Definition 3. worst-case regret = maximum regret over all possible environments.

### Example
- 상황: k-armed bandit with reward $X_t \in \{0, 1\}$ (stochastic Bernoulli bandit). 이 때의 action set은 $\mathcal{A} = \{ 1,2, ... k\}$ .
- $\mu_a = P(X_t = 1 | A_t = a)$ 라고 하면, optimal policy는 playing fixed action $a^* = \arg\max\limits_{a \in \mathcal{A}} \mu_a$  
- i.e, 이 문제의 natural competitor class $\Pi = \{\pi_1, ..., \pi_k \}$ , $\pi_i$ : 항상 action i를 선택하는 constant policy.
- 그러면 regret over n rounds : $$R_n = n \max\limits_{a \in \mathcal{A}} \mu_a - \mathbb{E}\left[\sum\limits_{t=1}^n X_t\right]$$
## Environment classes
- large environment class -> less knowledge by the learner
- large competitor class -> competitor가 많으면 그 중 가장 잘한 것과의 손실이 regret이므로 regret 줄이기 힘들어짐 -> regret is more demanding criteria.
- 적절한 environment class를 사용해야 함.
	- 크게 두 개를 사용: stochastic & adversarial
1. **'stochastic stationary bandits'**
	- 가정:
		- reward is **stochastic and stationary**.
		- = environment is restricted to generate the reward in response to each action from a distribution.
		- = reward dependent of action, and independent of previous actions and rewards.
	- 주로,
		- Assume the action set $\mathcal{A} \subset \mathbb{R}^d$  
		- and assume mean reward for choosing action $a$ follows a linear model is  $$X_t = <a,\theta> + \eta_t \quad for \quad \theta \in \mathbb{R}^d,\quad \eta_t \sim \mathcal{N}(1,0)$$
	- 우려점
		- 현실은 종종 stationary하지도 stochastic 하지도 않음
		- 한 round, 또는 몇 round만 violated되어도 best performing 하나?
2. **adversarial bandits**
	- 가정:
		- rewards are chosen without knowledge of the learner's actions
		- =reward 생성에 대한 아무 가정도 하지 않음
	- 주로,
		- competitor class 를 restrict하는 방식으로 해야meaningful 해짐. 그렇지 않으면 모래밭에서 바늘찾기 (매우 어려움)
		- so, set competitor class $\Pi$ to be set of constant policies
3. 기타 (Multi-objective )
	- 하나의 policy여도 enviornment에 따라 regret이 달라짐. -> multi-objective criterion
	- -> all possible environments에 대한 minimal regret을 고려하기 위해, average regret을 구할 수 있음
	- 이건 Bayesian 관점에서 environment class의 prior에 따라 average cumulative regret을 minimize하는 것


## Limitations of Bandit Framework
1. learner’s available choices and rewards tomorrow are not affected by their decisions today라고 가정. 즉, 미래 계획을 안함.
	- long-term planning 이 필요한 경우 Reinforcement Learning 필요
2. 매 라운드에서 보상이 관찰된다는 가정을 함.
	- 그렇지 않다면 Partial Monitoring 문제로 다룸
3. 상대도 전략적인(strategic agents) 환경은 다루지 않음
	- game thoery에서 다룸


## Applications
1. A/B testing
2. Advert placement
3. Recommendation Services
4. Network Routing
5. Dynamic Pricing
6. Waiting Problems
7. Resource Allocation
8. Tree Search



# Reference
Tor L., Csaba S., *Bandit Algorithm* 
